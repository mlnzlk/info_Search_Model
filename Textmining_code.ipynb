{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32925911",
   "metadata": {},
   "source": [
    "# 수만휘 네이버 카페 웹 크롤링 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import pyperclip\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 크롬 웹 드라이버의 경로를 설정\n",
    "driver_path = 'C:/Users/dlxod/chromedriver.exe'  # 크롬 드라이버 경로 입력\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # 브라우저 숨기기\n",
    "driver = webdriver.Chrome(driver_path, options=options)\n",
    "\n",
    "# 네이버 로그인 페이지 접속\n",
    "driver.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "\n",
    "# 로그인 정보\n",
    "login = {\n",
    "    \"id\": \"dlxodus890\",  # 네이버 아이디\n",
    "    \"pw\": \"wkftoddl12#\"  # 네이버 비밀번호\n",
    "}\n",
    "\n",
    "# 로그인 정보 입력 함수\n",
    "def clipboard_input(user_xpath, user_input):\n",
    "    temp_user_input = pyperclip.paste()\n",
    "\n",
    "    pyperclip.copy(user_input)\n",
    "    driver.find_element(By.XPATH, user_xpath).click()\n",
    "    ActionChains(driver).key_down(Keys.CONTROL).send_keys('v').key_up(Keys.CONTROL).perform()\n",
    "\n",
    "    pyperclip.copy(temp_user_input)\n",
    "    time.sleep(1)\n",
    "\n",
    "# id, pw 입력 후 클릭\n",
    "clipboard_input('//*[@id=\"id\"]', login.get(\"id\"))\n",
    "clipboard_input('//*[@id=\"pw\"]', login.get(\"pw\"))\n",
    "driver.find_element(By.XPATH, '//*[@id=\"log.login\"]').click()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "# 여기서부터 네이버 카페 크롤링 코드\n",
    "\n",
    "# 결과 저장 경로\n",
    "save_path = \"C:/Users/dlxod/수만휘_크롤링\"\n",
    "\n",
    "address_list = []\n",
    "page = 197\n",
    "\n",
    "# 카페 정보 \n",
    "cafe = {\n",
    "    'name': \"수능날만점시험지를휘날리자\",                                # 카페 이름\n",
    "    'page_link':\"https://cafe.naver.com/suhui\",                        # 주소\n",
    "    'board_link':f\"https://cafe.naver.com/ArticleList.nhn?search.clubid=10197921&search.menuid=3847&search.boardtype=L&search.totalCount=151&search.cafeId=10197921&search.page={page}\",             # 게시판 주소\n",
    "    'keywords':\"수시 결과 기록+수기\"                                    # 키워드 설정\n",
    "}\n",
    "\n",
    "# 카페 주소 입력\n",
    "driver.get(cafe['board_link'])\n",
    "\n",
    "# 키워드 수집 정보\n",
    "num_per_page = 15  # 페이지당 게시글 갯수 (default: 15개)\n",
    "\n",
    "l = True\n",
    "while l:\n",
    "    time.sleep(random.randint(0, 5))\n",
    "\n",
    "    driver.switch_to.frame(\"cafe_main\")  # iframe의 name 속성을 사용하여 전환\n",
    "    # 현재 페이지의 html 불러오기\n",
    "    r = driver.page_source\n",
    "    page_html = BeautifulSoup(r, \"html.parser\")\n",
    "\n",
    "    # iframe 내부에서 article-board m-tcol-c를 찾기 위해 수정\n",
    "    content = page_html.find_all(\"div\", class_=\"article-board m-tcol-c\")[1].find('table').find('tbody')\n",
    "    body = content.find_all(\"tr\")\n",
    "\n",
    "    # 게시글 정보 저장하기\n",
    "    for x in body:\n",
    "        temp_dict = {}\n",
    "        if x.find(\"div\", class_=\"board-number\") is not None:\n",
    "            temp_dict['no'] = x.find(\"div\", class_=\"inner_number\").text.strip()\n",
    "            temp_dict['title'] = x.find(\"div\", class_=\"board-list\").find(\"div\", class_=\"inner_list\").find('a', class_='article').text.strip().replace('  ', '').replace('\\n', '')\n",
    "            temp_dict['link'] = x.find(\"div\", class_=\"board-list\").find(\"div\", class_=\"inner_list\").find('a', class_='article').get('href')\n",
    "            temp_dict['name'] = x.find(\"td\", class_=\"td_name\").find('a', class_='m-tcol-c').text.strip()\n",
    "            temp_dict['date'] = x.find(\"td\", class_=\"td_date\").text.strip()\n",
    "            temp_dict['view'] = x.find(\"td\", class_=\"td_view\").text.strip()\n",
    "         \n",
    "            # ※ 게시글 들어가서 내용 크롤링하는 부분\n",
    "            driver.switch_to.default_content()  # iframe을 빠져나와 기본 컨텐츠로 전환\n",
    "            #driver.get(temp_dict['link'])\n",
    "            driver.get('https://cafe.naver.com' + temp_dict['link'])\n",
    "            time.sleep(random.randint(0, 15))\n",
    "            driver.switch_to.frame(\"cafe_main\")  # iframe 내부로 다시 진입\n",
    "            post_content = driver.page_source\n",
    "            post_html = BeautifulSoup(post_content, \"html.parser\")\n",
    "\n",
    "            element = post_html.find(\"div\", class_=\"se-component se-text se-l-default\")\n",
    "            if element is not None:\n",
    "                paragraphs = element.find_all(\"p\", class_=re.compile(r\"se-text-paragraph(se-text-paragraph-align-)?\"))\n",
    "                content = [p.text.strip() for p in paragraphs]\n",
    "                temp_dict['content'] = '\\n'.join(content)\n",
    "            else:\n",
    "                temp_dict['content'] = \"\"\n",
    "            \n",
    "            address_list.append(temp_dict)\n",
    "    print(\"(현재시각) \" + str(datetime.datetime.now()) + \": \" + str(page) + \" page done\")\n",
    "\n",
    "    # 다음 페이지로 넘어가기\n",
    "    driver.implicitly_wait(1)\n",
    "    try:\n",
    "        if page <= 299:  # 1페이지부터 300페이지까지 크롤링\n",
    "            # 페이지 이동\n",
    "            page += 1\n",
    "            driver.get(f\"https://cafe.naver.com/ArticleList.nhn?search.clubid=10197921&search.menuid=3847&search.boardtype=L&search.totalCount=151&search.cafeId=10197921&search.page={page}\")\n",
    "\n",
    "            # 대기 시간 추가\n",
    "            time.sleep(random.randint(0, 10))\n",
    "        else:\n",
    "            l = False  \n",
    "\n",
    "    except:\n",
    "        print(\"크롤링 중 오류 발생\")\n",
    "        l = False\n",
    "\n",
    "    finally:\n",
    "        address_df = pd.DataFrame(address_list)\n",
    "        address_df['idx_no'] = range(1, len(address_df) + 1)  # 조인할 키 값\n",
    "        address_df.to_csv(save_path + \".csv\", index=False, encoding='utf-8-sig')\n",
    "        print(\"(현재시각) \" + str(datetime.datetime.now()) + \": done\")\n",
    "\n",
    "if len(set(address_df['no'])) != len(address_df):\n",
    "    print(\"게시글 번호에 중복 존재\")\n",
    "print(\"검색게시글수:\", address_df.shape)\n",
    "display(address_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33b1e5",
   "metadata": {},
   "source": [
    "# 크롤링해온 csv파일 전처리 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "497235d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류된 데이터가 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# TextMining_수만휘_수시게시판_통합.csv에서 양식별로 column 찢기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('TextMining_수만휘_수시게시판_통합.csv',encoding='utf-8-sig')\n",
    "df['content'] = df['content'].astype(str)\n",
    "df['content'] = df['content'].str.strip()\n",
    "\n",
    "# 데이터프레임 생성\n",
    "new_df = pd.DataFrame(df, columns=df.columns)\n",
    "\n",
    "# 새로운 열을 추가하여 데이터를 분류\n",
    "new_columns = ['1. 입시결과 요약', '2. 출신고교 종류', '3. 내신/수능점수', '4. 스펙', '5. 후배들에게 전하고 싶은 말', '6. 수험 수기']\n",
    "new_df = pd.DataFrame(columns=new_columns)\n",
    "\n",
    "current_col_index = 0  # 현재 열 인덱스\n",
    "row_data = [''] * 6  # 초기화된 분류 데이터 리스트\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    content = str(row['content']).replace('\\n', ' ')  # 개행 문자 제거\n",
    "    \n",
    "    # 개행 문자 제거\n",
    "    content = re.sub(r'[^\\w\\s\\.\\-/.~]', '', content)\n",
    "    \n",
    "    sentences = re.split(r'(?<=\\d)\\.(?=\\s)', content)\n",
    "    \n",
    "    row_data = [''] * 6  # 각 행마다 row_data를 새로 초기화\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # 문장의 맨 끝에 있는 1개의 숫자 제거\n",
    "        sentence = re.sub(r'\\d$', '', sentence)\n",
    "        \n",
    "        if '입시결과' in sentence:\n",
    "            current_col_index = 0  # '1'이 나왔을 때 현재 열 인덱스를 0으로 설정\n",
    "            row_data[current_col_index] = sentence.replace('입시결과 요약합격 또는 불합격한 대학명/ 학과 / 전형', '')  # 1. 열에 해당하는 데이터에 문장 추가\n",
    "        elif '출신고교' in sentence:\n",
    "            current_col_index = 1  # '2'가 나왔을 때 현재 열 인덱스를 1로 설정\n",
    "            row_data[current_col_index] = sentence.replace('출신고교의 종류 / 성별성별이 영향을 주는 전형이 아니면 적지 않으셔도 됩니다', '')  # 2. 열에 해당하는 데이터에 문장 추가\n",
    "        elif '내신/수능' in sentence:\n",
    "            current_col_index = 2  # '3'이 나왔을 때 현재 열 인덱스를 2로 설정\n",
    "            row_data[current_col_index] = sentence.replace('내신/수능 점수', '')  # 3. 열에 해당하는 데이터에 문장 추가\n",
    "        elif '스펙' in sentence:\n",
    "            current_col_index = 3  # '4'가 나왔을 때 현재 열 인덱스를 3로 설정\n",
    "            row_data[current_col_index] = sentence.replace('스펙', '')  # 4. 열에 해당하는 데이터에 문장 추가\n",
    "        elif '후배들에게' in sentence:\n",
    "            current_col_index = 4  # '5'가 나왔을 때 현재 열 인덱스를 4로 설정\n",
    "            row_data[current_col_index] = sentence.replace('후배들에게 전하고 싶은 말', '')  # 5. 열에 해당하는 데이터에 문장 추가\n",
    "        elif '수험 수기' in sentence:\n",
    "            current_col_index = 5  # '6'이 나왔을 때 현재 열 인덱스를 5로 설정\n",
    "            row_data[current_col_index] = sentence.replace('수험 수기작성하시고 픈 분만 작성하셔도 됩니다.', '')  # 6. 열에 해당하는 데이터에 문장 추가\n",
    "        else:\n",
    "            row_data[current_col_index] += sentence  # 현재 열에 해당하는 데이터에 문장 추가\n",
    "    \n",
    "    new_df.loc[index] = row_data\n",
    "\n",
    "# 기존 DataFrame에 열 추가\n",
    "df[new_columns] = new_df[new_columns]\n",
    "\n",
    "# '1. 입시결과'나 '2. 스펙' 열 등 데이터가 없는 행을 삭제하는 코드\n",
    "df = df.drop(df[(df['1. 입시결과 요약'] == '') | (df['1. 입시결과 요약'] == ' ') | (df['2. 출신고교 종류'] == '') | (df['2. 출신고교 종류'] == ' ') | (df['3. 내신/수능점수'] == '') | (df['3. 내신/수능점수'] == ' ') | (df['4. 스펙'] == '') | (df['4. 스펙'] == ' ') | (df['5. 후배들에게 전하고 싶은 말'] == '') | (df['5. 후배들에게 전하고 싶은 말'] == ' ')].index)\n",
    "df.drop('6. 수험 수기', axis=1, inplace=True)\n",
    "df.to_csv('수만휘_전처리.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print('분류된 데이터가 성공적으로 저장되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16dd26fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 엑셀 파일 경로\n",
    "file_path = '수만휘_전처리.csv'\n",
    "\n",
    "# 엑셀 파일 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 9열에만 전처리를 적용할 경우\n",
    "target_column_school = 9\n",
    "# 10열에만 전처리를 적용할 경우\n",
    "target_column_score = 10\n",
    "\n",
    "# 전처리를 위한 함수 정의 10열\n",
    "def preprocess_score(value):\n",
    "    if isinstance(value, str):\n",
    "        # \"평균\", \"총\", \"전과목\", \"최종\" 처리\n",
    "        words = value.split()\n",
    "        result1 = []\n",
    "        for i, word in enumerate(words):\n",
    "            if word in ['평균', '총', '전과목', '최종', '전교과', '내신', '총내신', '총합']:\n",
    "                if i+1 < len(words):\n",
    "                    number = re.findall(r'\\d+\\.\\d+', words[i+1])\n",
    "                    result1.extend(number)\n",
    "\n",
    "        # 숫자5개 연속 처리\n",
    "        for word in words:\n",
    "            if len(word) == 5 and word.isdigit():\n",
    "                result1.append(word)\n",
    "\n",
    "        return ', '.join(result1) if result1 else ''\n",
    "    return ''\n",
    "\n",
    "# 전처리를 위한 함수 정의 9열\n",
    "def preprocess_school(value):\n",
    "    if isinstance(value, str):\n",
    "        words = value.split()  # 문장을 단어로 분리\n",
    "        result2 = []\n",
    "        for word in words:\n",
    "            if any(keyword in word for keyword in ['일반', '과학', '특성화', '자사', '외','외국어']):\n",
    "                if '일반' in word:\n",
    "                    word = '일반고'\n",
    "                elif '과학' in word:\n",
    "                    word = '과학고'\n",
    "                elif '특성화' in word:\n",
    "                    word = '특성화고'\n",
    "                elif '자사' in word:\n",
    "                    word = '자사고'\n",
    "                elif '외' in word or '외국어' in word:\n",
    "                    word = '외고'\n",
    "                result2.append(word)\n",
    "        if result2:\n",
    "            return ', '.join(result2)\n",
    "    return ''\n",
    "\n",
    "\n",
    "column_name1 = df.columns[target_column_score]\n",
    "column_name2 = df.columns[target_column_school]\n",
    "\n",
    "# 전처리된 값을 저장할 새로운 열 이름\n",
    "new_column_name1 = \"preprocessed_score\"\n",
    "new_column_name2 = \"preprocessed_school\"\n",
    "\n",
    "# 새로운 열 생성 및 전처리된 값 저장\n",
    "#df[new_column_name1] = df[column_name1].apply(preprocess_score)\n",
    "#df[new_column_name2] = df[column_name2].apply(preprocess_school)\n",
    "df['3. 내신/수능점수'] = df['3. 내신/수능점수'].apply(preprocess_score)\n",
    "df['2. 출신고교 종류'] = df['2. 출신고교 종류'].apply(preprocess_school)\n",
    "\n",
    "# 새로운 CSV 파일로 저장\n",
    "df.to_csv('수만휘_전처리_내신&학교.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"추출이 완료되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2e6e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수능 점수 제거 = 내신 점수만 남김\n",
    "# 이후 평균 내기 위해서. 그리고 애초에 여기는 \"수시 합격 수기 게시판\"이라 내신 점수만 있어도 된다고 판단.\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "data = pd.read_csv('수만휘_전처리_내신&학교.csv')\n",
    "\n",
    "# 정규식 패턴으로 3개 이상 연속된 숫자 형태의 값 찾기\n",
    "pattern = r'\\d{3,}'\n",
    "data['3. 내신/수능점수'] = data['3. 내신/수능점수'].apply(lambda x: re.sub(pattern, '', str(x).replace(',', '')) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# 결과를 새로운 CSV 파일로 저장하기\n",
    "data.to_csv('수만휘_전처리_내신&학교.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59652f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "출력 파일이 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "csv_file = \"수만휘_전처리_내신&학교.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "def extract_string_with_words(sentence, conditions):\n",
    "    matches = []\n",
    "    for condition in conditions:\n",
    "        word1 = condition[\"word1\"]\n",
    "        word2 = condition[\"word2\"]\n",
    "        pattern = r\"\\b\" + word1 + r\"\\s*(?:\\d+\\s*)?\\w*\\s*\" + word2 + r\"\\b\"  # 수정된 정규표현식\n",
    "        match = re.search(pattern, sentence, flags=re.IGNORECASE)  # 대소문자 무시하도록 flags 추가\n",
    "        if match:\n",
    "            extracted_string = match.group().strip()  # 매치된 문자열에서 앞뒤 공백 제거\n",
    "            matches.append(extracted_string)\n",
    "    return matches\n",
    "\n",
    "# 추출할 단어 설정\n",
    "conditions = [\n",
    "    {\"word1\": \"생기부\", \"word2\": \"장\"},\n",
    "    {\"word1\": \"독서\", \"word2\": \"권\"},\n",
    "    {\"word1\": \"봉사활동\", \"word2\": \"시간\"},\n",
    "    {\"word1\": \"봉사\", \"word2\": \"시간\"},\n",
    "    {\"word1\": \"상장\", \"word2\": \"개\"},\n",
    "    {\"word1\": \"수상\", \"word2\": \"개\"}\n",
    "]\n",
    "\n",
    "# 추출 작업 수행 및 결과 저장\n",
    "target_column = 11  # 추출할 열의 인덱스 (0부터 시작)\n",
    "column_name = df.columns[target_column]  # 추출할 열의 이름\n",
    "extracted_data = []\n",
    "for condition in conditions:\n",
    "    word1 = condition[\"word1\"]\n",
    "    word2 = condition[\"word2\"]\n",
    "    extracted_values = df[column_name].apply(lambda x: extract_string_with_words(str(x), conditions))\n",
    "    extracted_data.append(extracted_values)\n",
    "\n",
    "# 추출한 값들을 기존 데이터프레임에 새로운 열로 추가\n",
    "# for i, condition in enumerate(conditions):\n",
    "#     word1 = condition[\"word1\"]\n",
    "#     word2 = condition[\"word2\"]\n",
    "#     new_column_name = f\"{word1}_{word2}\"\n",
    "#     df[new_column_name] = extracted_data[i]\n",
    "new_column_name = \"스펙_핵심_추출\"\n",
    "df[new_column_name] = extracted_data[0]\n",
    "\n",
    "# 수정된 데이터프레임을 CSV 파일로 출력하여 저장\n",
    "output_csv_file = \"수만휘_전처리_내신&학교&스펙.csv\"\n",
    "df.to_csv(output_csv_file, index=False, encoding='utf-8-sig')\n",
    "print(\"출력 파일이 생성되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46d69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def count_words_in_csv(input_file, target_column_index, words):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "\n",
    "    counts = []\n",
    "    for row in rows:\n",
    "        cell = row[target_column_index]\n",
    "        count = 0\n",
    "        for word in words:\n",
    "            count += cell.count(word)\n",
    "        counts.append(count)\n",
    "\n",
    "    output_file = os.path.splitext(os.path.basename(input_file))[0] + '_output.csv'\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row, count in zip(rows, counts):\n",
    "            row.append(str(count))\n",
    "            writer.writerow(row)\n",
    "\n",
    "input_file = '수만휘_전처리_내신&학교&스펙.csv'\n",
    "target_column_index = 11\n",
    "words = ['동아리', '상장', '수상', '독서', '봉사', '반장', '회장', '자소서', '학생회', '생기부', '세특', '임원', '프로젝트', '활동', '원서']\n",
    "\n",
    "count_words_in_csv(input_file, target_column_index, words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82103776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement summarize (from versions: none)\n",
      "ERROR: No matching distribution found for summarize\n"
     ]
    }
   ],
   "source": [
    "!pip install summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17818d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: summa in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from summa) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from scipy>=0.19->summa) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7a603e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement summa.summarizer (from versions: none)\n",
      "ERROR: No matching distribution found for summa.summarizer\n"
     ]
    }
   ],
   "source": [
    "!pip install summa.summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fa63b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "df = pd.read_csv('수만휘_전처리_내신&학교&스펙_output.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 열 이름 변경\n",
    "df = df.rename(columns={'0': '중요단어 카운팅 수'})\n",
    "\n",
    "# 텍스트 요약\n",
    "summary = []\n",
    "for text in df['4. 스펙']:\n",
    "    if isinstance(text, str):  # 문자열인지 확인\n",
    "        words = text.split()  # 단어를 공백 기준으로 분리\n",
    "        num_words = len(words)\n",
    "        if num_words >= 20:  # 20단어 이상인 경우 요약 진행\n",
    "            summarized_text = summarize(text, words=30)  # 30단어로 요약\n",
    "            summarized_text = summarized_text.replace('\\n', ' ').replace('/', ' ')\n",
    "        else:\n",
    "            summarized_text = text.replace('\\n', ' ').replace('/', ' ')  # 20단어 이하인 경우 원문 그대로 출력\n",
    "    else:\n",
    "        summarized_text = ''  # 빈 데이터나 문자열이 아닌 경우 처리\n",
    "\n",
    "    summary.append(summarized_text)\n",
    "\n",
    "# 요약 결과를 새로운 열로 추가\n",
    "df['4. 요약'] = summary\n",
    "\n",
    "df.to_csv('수만휘_전처리_내신&학교&스펙&요약.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd76ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('수만휘_전처리_내신&학교&스펙&요약.csv', encoding='utf-8-sig')\n",
    "\n",
    "# '5. 후배들에게 전하고 싶은 말' 열의 텍스트 요약\n",
    "summary = []\n",
    "for text in df['5. 후배들에게 전하고 싶은 말']:\n",
    "    if isinstance(text, str):  # Check if the text is a string\n",
    "        words = text.split()  # 단어를 공백 기준으로 분리\n",
    "        num_words = len(words)\n",
    "        if num_words >= 20:  # 20단어 이상인 경우 요약 진행\n",
    "            summarized_text = summarize(text, words=20)  # 20단어로 요약\n",
    "            summarized_text = summarized_text.replace('\\n', ' ').replace('/', ' ')\n",
    "        else:\n",
    "            summarized_text = text.replace('\\n', ' ').replace('/', ' ')  # 20단어 이하인 경우 원문 그대로 출력\n",
    "    else:\n",
    "        summarized_text = ''  # Handle empty or non-string data\n",
    "\n",
    "    summary.append(summarized_text)\n",
    "\n",
    "# 요약 결과를 새로운 열로 추가\n",
    "df['5. 요약'] = summary\n",
    "\n",
    "df.to_csv('수만휘_전처리_내신&학교&스펙&요약.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f44963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 줄넘김으로 대학 분류하는 코드. 기존 열 추가 코드에서 행 복사->입시결과 열 값만 다르게 넣는 걸로 수정함.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "data = pd.read_csv('수만휘_전처리_내신&학교&스펙&요약.csv')\n",
    "\n",
    "# \"1. 입시결과 요약\" 열에서 텍스트 끊어서 \"new_col\"에 저장\n",
    "results = []\n",
    "summary_column = data['1. 입시결과 요약']\n",
    "for text in summary_column:\n",
    "    if pd.notnull(text):  # NaN 값이 아닌 경우에만 처리\n",
    "        # 대나 대학교가 포함된 단어 앞에서 줄넘김 처리\n",
    "        lines = []\n",
    "        current_line = ''\n",
    "        for word in text.split():\n",
    "            if '대' in word or '대학교' in word:\n",
    "                if current_line:\n",
    "                    lines.append(current_line)\n",
    "                current_line = word\n",
    "            else:\n",
    "                current_line += ' ' + word\n",
    "        if current_line:\n",
    "            lines.append(current_line)\n",
    "        formatted_text = '\\n'.join(lines)\n",
    "        # 맨 마지막 줄에 줄넘김 추가\n",
    "        if formatted_text:\n",
    "            formatted_text += '\\n'\n",
    "        results.append(formatted_text)\n",
    "    else:\n",
    "        results.append('')  # 결측치인 경우 빈 문자열로 처리\n",
    "\n",
    "# \"new_col\" 열을 DataFrame에 추가\n",
    "data['new_col'] = results\n",
    "\n",
    "# 'new_col'열이 결측치인 경우, 해당 row 제거\n",
    "data = data[data['new_col'] != '']\n",
    "\n",
    "# 새로운 칼럼 생성\n",
    "max_columns = 10  # 최대 칼럼 개수 제한\n",
    "num_lines = data['new_col'].str.count('\\n')  # 줄넘김 개수\n",
    "num_lines[num_lines > max_columns] = max_columns  # 최대 칼럼 개수 제한\n",
    "new_columns = [f\"입시결과{i+1}\" for i in range(max_columns)]  # 칼럼 이름 생성\n",
    "\n",
    "# 값 저장\n",
    "new_data = pd.DataFrame()\n",
    "for i, row in data.iterrows():\n",
    "    for j in range(max_columns):\n",
    "        new_row = row.copy()\n",
    "        if j < num_lines[i]:\n",
    "            new_row['입시결과'] = row['new_col'].split('\\n')[j]\n",
    "        else:\n",
    "            new_row['입시결과'] = ''\n",
    "        new_data = pd.concat([new_data, new_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "# '입시결과'열이 결측치인 경우, 해당 row 제거\n",
    "#new_data = new_data.dropna(subset=['입시결과'])\n",
    "# 결측치인 row 제거\n",
    "new_data = new_data[new_data['입시결과'] != '']\n",
    "\n",
    "# 새로운 CSV 파일로 저장\n",
    "new_data.to_csv('수만휘_전처리_내신&학교&스펙&요약&대학분류.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0182c",
   "metadata": {},
   "source": [
    "[대학 나누기 + 단어 태깅까지 완료한 코드]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93f341c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "df = pd.read_csv('수만휘_전처리_내신&학교&스펙&요약&대학분류.csv')\n",
    "\n",
    "# 태깅 결과를 저장할 리스트 초기화\n",
    "universities = []\n",
    "departments = []\n",
    "results = []\n",
    "\n",
    "# 각 텍스트에서 '대학교', '학과명', '합불여부' 태깅\n",
    "for text in df['입시결과']:\n",
    "    # 태깅 결과를 저장할 딕셔너리 초기화\n",
    "    tags = {\n",
    "        '대학명': None,\n",
    "        '학과명': None,\n",
    "        '합불여부': None\n",
    "    }\n",
    "\n",
    "    # '대학교' 태깅\n",
    "    match = re.search(r'\\b(\\w*대학\\w*|.*대)\\b', text)\n",
    "    if match:\n",
    "        tags['대학명'] = match.group()\n",
    "\n",
    "    # '학과명' 태깅\n",
    "    match = re.search(r'\\b(\\w*과\\w*|\\w*학과\\w*|\\w*학부\\w*)\\b', text)\n",
    "    if match:\n",
    "        department = match.group()\n",
    "        if '교과' not in department and '종합' not in department:\n",
    "            tags['학과명'] = department\n",
    "\n",
    "    # '합불여부' 태깅\n",
    "    match = re.search(r'\\b(\\w*합(?!.*[대학교|학과명])\\w*)|(\\w*탈(?!.*[대학교|학과명])\\w*)|(\\w*불(?!.*[대학교|학과명])\\w*)|(\\w*예(?!.*[대학교|학과명])\\w*)|(\\w*최(?!.*[대학교|학과명])\\w*)|(\\w*미(?!.*[대학교|학과명])\\w*)|(\\w*떨(?!.*[대학교|학과명])\\w*)\\b', text)\n",
    "    if match:\n",
    "        department = match.group()\n",
    "        if '종합' not in department:\n",
    "            tags['합불여부'] = department\n",
    "\n",
    "    # 태깅 결과를 리스트에 저장\n",
    "    universities.append(tags['대학명'])\n",
    "    departments.append(tags['학과명'])\n",
    "    results.append(tags['합불여부'])\n",
    "\n",
    "# 태깅 결과로 새로운 열을 추가한 데이터프레임 생성\n",
    "df['대학명'] = universities\n",
    "df['학과명'] = departments\n",
    "df['합불여부'] = results\n",
    "\n",
    "# 새로운 CSV 파일로 저장\n",
    "df.to_csv('수만휘_전처리_내신&학교&스펙&요약&대학분류&태깅.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb63214d",
   "metadata": {},
   "source": [
    "'광운대학교' 라고 검색하는 것보다 '광운'이라고 검색해야 더 잘 나옵니다. 해당 단어 포함하고 있는 row을 검색하게 만들었기 때문데, 광운대학교라고 치면 \"광운대학교\"라는 단어가 그대로 박힌 row밖에 안 뽑아줘요. \n",
    "다른 학교로 생각하셔도 똑같아요. 고려대 검색하려면 \"고려/경영\" 또는 \"고려/경영학과\" 이런 식으로 검색하시는 게 좋아용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3b514a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결측치 개수: 5890\n",
      "전체 row 개수: 14919\n",
      "Index(['no', 'title', 'link', 'name', 'date', 'view', 'content', 'idx_no',\n",
      "       '1. 입시결과 요약', '2. 출신고교 종류', '3. 내신/수능점수', '4. 스펙', '5. 후배들에게 전하고 싶은 말',\n",
      "       '스펙_핵심_추출', '중요단어 카운팅 수', '4. 요약', '5. 요약', 'new_col', '입시결과', '대학명',\n",
      "       '학과명', '합불여부'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 결측치 확인하는 코드\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # CSV 파일 불러오기\n",
    "# data = pd.read_csv('수만휘_전처리_내신&학교&스펙&대학분류&태깅.csv')\n",
    "# 원하는 col 결측치 보기\n",
    "missing_values = df['학과명'].isnull().sum()\n",
    "print(f\"결측치 개수: {missing_values}\")\n",
    "# 전체 col 갯수 보기\n",
    "total_rows = len(df)\n",
    "print(f\"전체 row 개수: {total_rows}\")\n",
    "\n",
    "#칼럼 Name 출력\n",
    "column_names = df.columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02e509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Selenium in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from Selenium) (0.10.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from Selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from Selenium) (2023.5.7)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from Selenium) (1.26.15)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (1.1.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (1.3.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (3.4)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (1.15.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (22.1.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (1.10)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->Selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->Selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->Selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dlxod\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->Selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n",
      "'apt'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    }
   ],
   "source": [
    "!pip install Selenium\n",
    "!apt-get update \n",
    "!apt install chromium-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d2e0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from konlpy) (4.9.2)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from konlpy) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from konlpy) (1.24.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lotly (c:\\users\\dlxod\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da17331e",
   "metadata": {},
   "source": [
    "# 검색 모델(검색 기능 구현)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e52f03d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:134: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:134: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원하시는 대학명과 학과를 입력해주세요 ex)광운/정보융합 : 상명/컴퓨터\n",
      "태깅된 행을 찾았습니다. 해당하는 열의 개수: 7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlxod\\AppData\\Local\\Temp\\ipykernel_5056\\829330758.py:89: DeprecationWarning: use options instead of chrome_options\n",
      "  wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
      "C:\\Users\\dlxod\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
      "C:\\Users\\dlxod\\AppData\\Local\\Temp\\ipykernel_5056\\829330758.py:134: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if sentence is not '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "[상명/컴퓨터에 게시글들에 대한 통합 결과입니다]\n",
      "상명/컴퓨터의 합격 수기를 작성한 이들의 내신 평균 점수:3.03\n",
      "상명/컴퓨터의 합격 수기를 작성한 이들의 고등학교 유형 분류 결과: \n",
      "일반고: 4개 (57.14%)\n",
      "자사고: 0개 (0.00%)\n",
      "특성화고: 0개 (0.00%)\n",
      "과학고: 0개 (0.00%)\n",
      "외고: 0개 (0.00%)\n",
      "상명/컴퓨터 사람들의 스펙 핵심 키워드 : \n",
      "['학급', '봉사', '대부분', '활동', '과학', '관련', '세특', '최대한', '동아리', '과목']\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "\n",
      "[상명/컴퓨터에 대한 1번째 게시글입니다.]\n",
      "1. 글 제목: <3점중반의 컴공도전기>\n",
      "2. 작성 날짜: 2021.12.22.\n",
      "3. 출신 고교: 일반고\n",
      "4. 내신 점수: 3.45\n",
      "5. 입시 결과: \n",
      "세종대학교 지능기전공학부 창의인재 예비51번-예비31번-예비28번\n",
      "성신여자대학교 컴퓨터공학과 학교생활우수자 예비1번 - 1차 추합\n",
      "상명대학교 컴퓨터과학과 상명인재 1차불합\n",
      "동덕여자대학교 컴퓨터학과 창의리더 예비9번 -2차 추합\n",
      "경기대학교 컴퓨터공학과 kgu 예비9번 -1차 추합\n",
      "한신대 컴퓨터공학과 교과1 최초합격\n",
      "대학어디가 기준 2.5\n",
      "6. 스펙 핵심: ['생기부 16장']\n",
      "6. 스펙 요약: 독서 47권전공관련 12권정도봉사 약 70시간 생기부 16장 꽉 채움 학급임원 2번정규 자율동아리 임원 전공관련 상은 없는데 그나마 수학사진전..그래도 1학기에 상 하나씩은 꼭 있었음  컴퓨터공학과를 가겠다고 다짐한 건 2학년 초~중반즈음이였어서 컴공관련 활동이 빈약했고 남들보다 부족한 부분을 어떻게든 채우고자 했습니다.\n",
      "7. 후배들에게 전하고 싶은 말: nan\n",
      "\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "\n",
      "[상명/컴퓨터에 대한 2번째 게시글입니다.]\n",
      "1. 글 제목: <일반고 내신2.4 / 컴공,인공지능학과 / 3개 최초합>\n",
      "2. 작성 날짜: 2023.01.09.\n",
      "3. 출신 고교: 일반고\n",
      "4. 내신 점수: 2.4\n",
      "5. 입시 결과: \n",
      "한양대 컴퓨터소프트웨어학부 종합일반 - 노예비 불합격\n",
      "동국대 AI소프트웨어융합 두드림소프트웨어 - 1차합 최종 노예비 불합격\n",
      "인하대 컴퓨터공학과 인하참인재 - 최초합\n",
      "서울과학기술대학교 인공지능응용학과 첨단인재전현 - 최초합\n",
      "단국대 컴퓨터공확과 지역균형선발 - 예비16 불합격\n",
      "상명대 컴퓨터과학과 학생부교과전형 - 최초합\n",
      "인하대\n",
      "과기대 고민하다\n",
      "과기대 가기로 했어요ㅎㅎ\n",
      "6. 스펙 핵심: []\n",
      "6. 스펙 요약: 1학년- 딱히.. 꿈이 명확하지 않아 이런저런 활동을 했어요 2학년- 프로그래밍동아리 학급회장 3학년- 프로그래밍동아리 회장  그 외 모든 세특이나 진로 활동을 인공지능이랑 엮어서 했어요  수상경력1학년1학기~3학년1학기 -과학TED발표대회 -통계대회 -수학사고력대회 -프로그래밍경진대회 -소프트웨어경진대회  독서- 54권 봉사- 36시간 생기부- 20장  \n",
      "7. 후배들에게 전하고 싶은 말: - 학종은 정말 예상할 수 없는 것. 정말 낮은 대학교만 추천하시면서 절대 안될 거라는 식으로 계속 말씀하셨습니다.\n",
      "\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "\n",
      "[상명/컴퓨터에 대한 3번째 게시글입니다.]\n",
      "1. 글 제목: <서울 일반고 3.2 수시최종결과(5합 1떨)>\n",
      "2. 작성 날짜: 2022.12.26.\n",
      "3. 출신 고교: 일반고\n",
      "4. 내신 점수: 3.24\n",
      "5. 입시 결과: \n",
      "세종대 지능기전공학과 / 학종 / 예비15 - 3차추합 최종등록\n",
      "세종대 반도체시스템공학과 / 학종 / 예비5 - 3차추합\n",
      "상명대 컴퓨터과학과 / 학종 / 1차광탈\n",
      "서울여대 소프트웨어융합학과 / 학종 / 예비3 - 1차추합\n",
      "한성대\n",
      "IT공대 / 학종 / 최초합장학금\n",
      "한성대\n",
      "IT공대 / 교과 / 예비5 - 1차추합\n",
      "6. 스펙 핵심: []\n",
      "6. 스펙 요약: - 1학년 2학년 3학년 학급회장 및 IOT동아리 부회장 - 상장은 물리 화학 기하 과학캠프 등등 골고루 5개 선정해서 기재함 총 상장은 21개 - 과목선택 물1화1생1지1 물2화2 미적기하프로그래밍 등 필요한과목 다 선택했고 최대한 소프트웨어랑 연결하고자 노력함 연결할게 없는 과목들은 탐구력과 수행력을 보여주고자 노력함 - SW아카데미등 IT관련 프로그램 거의 다 참여.\n",
      "7. 후배들에게 전하고 싶은 말: 저는 솔직히 종합의 이득을 본 사람이라고 생각해요. 생기부 채워두는거 나쁘지않다고 생각해요.\n",
      "\n",
      "ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "\n",
      "[상명/컴퓨터의 3개의 전체 요약본 입니다.]\n",
      "요약문: 1학년 딱히 꿈이 명확하지 않아 이런저런 활동을 했어요 2학년 프로그래밍동아리 학급회장 3학년 프로그래밍동아리 회장 그 외 모든 세특이나 진로 활동을 인공지능이랑 엮어서 했어요 수상경력1학년1학기3학년1학기 과학발표대회 통계대회 수학사고력대회 프로그래밍경진대회 소프트웨어경진대회 독서 54권 봉사 36시간 생기부 20장\n",
      "\n",
      "요약문: 생기부 14장 홈페이지나 앱 개발같은 컴공관련 심화활동 한개도없음 학술활동 수상 물리경시동상 1개 독서 20권정도 대부분 전공관련 봉사 거의안함 2년 학급임원 2학년 말까지 대학생각 1도 없었기에 생기부 무지성으로 대충 채워서 영양가 별로없음 3년동안 쭉 컴공쪽 희망\n",
      "\n",
      "요약문: 독서 47권전공관련 12권정도봉사 약 70시간 생기부 16장 꽉 채움 학급임원 2번정규자율동아리 임원 전공관련 상은 없는데 그나마 수학사진전그래도 1학기에 상 하나씩은 꼭 있었음 컴퓨터공학과를 가겠다고 다짐한 건 2학년 초중반즈음이였어서 컴공관련 활동이 빈약했고 남들보다 부족한 부분을 어떻게든 채우고자 했습니다 비록 2년동안 토론 동아리였지만 최대한 동아리 내에서 인공지능자율주행차 등 주제를 접목시켜 활동했습니다 과학수학세특 그리고 국어영어같이 상대적으로 관련이 적은 과목이더라도 최대한 컴퓨터공학과 관련된 주제들을 이용해 발표했습니다 12학년 때 코딩활동을 한 게 전혀 없어서 3학년때 작게라도 해보자했습니다 세특에 쓰인 코딩활동은 대부분 스크래치앱인벤터같은 블록코딩이였고 큰 활동 두개정도는 파이썬으로 했습니다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여기가 \"학교명/학과명\"으로 검색하는 기능 구현 코드\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "df = pd.read_csv('수만휘_전처리_내신&학교&스펙&요약&대학분류&태깅.csv')\n",
    "\n",
    "# 사용자 입력 받기\n",
    "user_input = input(\"원하시는 대학명과 학과를 입력해주세요 ex)광운/정보융합 : \")\n",
    "\n",
    "# 대학명과 학과명 추출\n",
    "match = re.search(r'(\\w+)/(\\w+)', user_input)\n",
    "if match:\n",
    "    university = match.group(1)\n",
    "    department = match.group(2)\n",
    "else:\n",
    "    print(\"입력이 올바르지 않습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 대학명과 학과명이 포함된 행 찾기\n",
    "filtered_df = df[df['대학명'].str.contains(university) & df['학과명'].str.contains(department)]\n",
    "\n",
    "# 결과 출력\n",
    "if len(filtered_df) > 0:\n",
    "    print(\"태깅된 행을 찾았습니다. 해당하는 열의 개수:\", len(filtered_df))\n",
    "    print(\"\")\n",
    "else:\n",
    "    print(\"일치하는 태깅된 행이 없습니다.\")\n",
    "\n",
    "##########################################################################################################\n",
    "# 여기부터 희원이 키워드 추출 코드\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import re\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Twitter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "df = pd.read_csv('수만휘_전처리_내신&학교&스펙&요약&대학분류&태깅.csv', encoding='utf-8-sig')\n",
    "\n",
    "filtered_df.loc[:, '4. 스펙'] = filtered_df['4. 스펙'].fillna('').apply(lambda x: re.sub('[^가-힣0-9\\s]', '', x))\n",
    "\n",
    "# '4. 스펙' 열의 내용을 하나의 문자열로 합치기\n",
    "spec_string = ' '.join(filtered_df['4. 스펙'].astype(str).tolist())\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "# 형태소 분석 수행\n",
    "morphs = okt.nouns(spec_string)\n",
    "# 단어 카운트\n",
    "word_counter = Counter(morphs)\n",
    "\n",
    "# 가장 많이 나온 상위 10개의 단어 추출\n",
    "top_10_words = word_counter.most_common(10)\n",
    "top_10_words = [word[0] for word in top_10_words]\n",
    "# 상위 10개 단어\n",
    "sentence = ' '.join(top_10_words)\n",
    "\n",
    "#입력한 단어 리스트(words)에 해당하는 문장들을 filtered_df의 '4. 스펙' 열에서 추출하는 작업을 수행\n",
    "# 입력한 단어 리스트\n",
    "words = ['동아리', '상장', '수상', '봉사', '반장', '회장', '자소서', '학생회', '세특', '임원', '프로젝트', '활동', '원서', '상', '대회', '부장', '확장', '활동', '발표', '주제', '분야', '수상경력', '보고서']\n",
    "\n",
    "# 단어 리스트에 해당하는 문장 추출\n",
    "selected_sentences = []\n",
    "for sentence in filtered_df['4. 스펙']:\n",
    "    if any(word in sentence for word in words):\n",
    "        sentence = sentence.replace('\\n', '.').replace('  ', ' ')\n",
    "        selected_sentences.append(sentence)\n",
    "        \n",
    "sentences = selected_sentences\n",
    "# 문장들을 담은 리스트로부터 데이터프레임 생성\n",
    "spec_data = pd.DataFrame({'Sentences': sentences})\n",
    "\n",
    "# 부산대 맞춤법 검사기 - 1차 교정\n",
    "# 브라우저 주소를 통해 \"부산대 맞춤법 교정기\" 페이지로 이동해서 교정을 시행한 후, 교정된 텍스트을 가져오는 코드\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
    "def spell_checking(sentence, text_list):\n",
    "    for i in (range(len(sentence))):\n",
    "        time.sleep(0.5)\n",
    "        wd.get('http://speller.cs.pusan.ac.kr/')\n",
    "        try:\n",
    "            wd.find_element_by_xpath('//*[@id=\"text1\"]').send_keys(sentence[i])\n",
    "            wd.find_element_by_xpath('//*[@id=\"btnCheck\"]').click()\n",
    "            time.sleep(1)\n",
    "            entity_num = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    wd.find_element_by_xpath('//*[@id=\"tdReplaceWord_'+str(entity_num)+'\"]/ul/li/a').click()\n",
    "                    entity_num += 1\n",
    "                except:\n",
    "                    break\n",
    "            texts = wd.find_element_by_xpath('//*[@id=\"tdCorrection1stBox\"]').text\n",
    "            text_list.append(texts)\n",
    "        except:\n",
    "            text_list.append(sentence[i])\n",
    "\n",
    "test_spell_list = []\n",
    "\n",
    "spell_checking(spec_data['Sentences'], test_spell_list)\n",
    "\n",
    "# 문장을 문장단위로 나누는 함수\n",
    "# test_spell_list에는 맞춤법 교정이 적용된 문장들이 저장되어 있음\n",
    "def split_sentences(text):\n",
    "    sentences = text.split('.')\n",
    "    cleaned_sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return cleaned_sentences\n",
    "\n",
    "# 문장별로 나누기\n",
    "sentences = []\n",
    "for text in test_spell_list:\n",
    "    sentences.extend(split_sentences(text))\n",
    "    \n",
    "twitter = Twitter()\n",
    "\n",
    "#불용어제거\n",
    "stopwords = ['학년' , \"때\", \"개\", \"권\", \"시간\", \"나\", \"우리\", \"저희\", \"따라\", \"의해\", \"을\", \"를\", \"에\", \"의\", \"가\", \"장\"]\n",
    "\n",
    "def get_nouns(sentences):\n",
    "    nouns = []\n",
    "    for sentence in sentences:\n",
    "        if sentence is not '':\n",
    "            nouns.append(' '.join([noun for noun in twitter.nouns(str(sentence))\n",
    "                                  if noun not in stopwords and len(noun) > 1]))\n",
    "    return nouns\n",
    "\n",
    "nouns = get_nouns(sentences)\n",
    "\n",
    "# TF-IDF 모델 생성 및 그래프 생성\n",
    "tfidf = TfidfVectorizer()\n",
    "cnt_vec = CountVectorizer()\n",
    "graph_sentence = []\n",
    "\n",
    "def build_sent_graph(sentence):\n",
    "    tfidf_mat = tfidf.fit_transform(sentence).toarray()\n",
    "    graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "    return graph_sentence\n",
    "\n",
    "sent_graph = build_sent_graph(nouns)\n",
    "\n",
    "def build_words_graph(sentence):\n",
    "    cnt_vec_mat = normalize(cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "    vocab = cnt_vec.vocabulary_\n",
    "    return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}\n",
    "\n",
    "words_graph, idx2word = build_words_graph(nouns)\n",
    "\n",
    "# TextRank 구현\n",
    "# 각 단어의 중요도를 계산하여 반환\n",
    "def get_ranks(graph, d=0.85): # d = damping factor\n",
    "    A = graph\n",
    "    matrix_size = A.shape[0]\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0 # diagonal 부분을 0으로\n",
    "        link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "        if link_sum != 0:\n",
    "            A[:, id] /= link_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "\n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "    return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "sent_rank_idx = get_ranks(sent_graph)  #sent_graph : sentence 가중치 그래프\n",
    "\n",
    "sorted_sent_rank_idx = sorted(sent_rank_idx, key=lambda k: sent_rank_idx[k], reverse=True)\n",
    "\n",
    "#앞에 있는 인덱스의 문장일수록 점수가 가장 높게 매겨졌음을 나타냄.\n",
    "word_rank_idx = get_ranks(words_graph)\n",
    "sorted_word_rank_idx = sorted(word_rank_idx, key=lambda k: word_rank_idx[k], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "def keywords(word_num=10):\n",
    "    keywords = []\n",
    "    index=[]\n",
    "    for idx in sorted_word_rank_idx[:word_num]:\n",
    "        index.append(idx)\n",
    "\n",
    "    #index.sort()\n",
    "    for idx in index:\n",
    "        keywords.append(idx2word[idx])\n",
    "\n",
    "    print(f\"{user_input} 사람들의 스펙 핵심 키워드 : \")\n",
    "    print(keywords)\n",
    "# keywords()\n",
    "\n",
    "##########################################################################################################\n",
    "# 이건 지원이가 짠 3. 내신/수능 점수 열에서 평균 구하는 코드\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# 숫자 값 추출 함수 정의\n",
    "def extract_number(value):\n",
    "    try:\n",
    "        number = re.findall(r'[+-]?\\d+(?:\\.\\d+)?', str(value))\n",
    "        if len(number) > 0:\n",
    "            return float(number[0])\n",
    "        else:\n",
    "            return np.nan\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# 각 셀에 있는 숫자 값 추출하여 리스트에 저장\n",
    "numbers = []\n",
    "for value in filtered_df['3. 내신/수능점수']:\n",
    "    number = extract_number(value)\n",
    "    if not np.isnan(number):\n",
    "        numbers.append(number)\n",
    "\n",
    "# 평균 계산\n",
    "average = np.mean(numbers)\n",
    "\n",
    "# 평균 값을 소수점 둘째 자리에서 반올림하여 출력\n",
    "# rounded_average 변수에 검색한 학교/학과 사람들의 평균 내신 점수 넣음.\n",
    "rounded_average = round(average, 2)\n",
    "# print(\"평균:\", rounded_average)\n",
    "\n",
    "# 여기서부터는 filtered_df에 대한 고등학교 비율 계산 코드\n",
    "import pandas as pd\n",
    "\n",
    "# filtered_df 데이터프레임에서 학교 유형 분류 및 카운트\n",
    "school_types = ['일반고', '자사고', '특성화고', '과학고', '외고']\n",
    "total_count = len(filtered_df)\n",
    "type_counts = filtered_df['2. 출신고교 종류'].value_counts()\n",
    "\n",
    "# '외고'와 '외국어고'를 하나로 합치기\n",
    "type_counts['외고'] = type_counts.get('외고', 0) + type_counts.get('외국어고', 0)\n",
    "if '외국어고' in type_counts:\n",
    "    del type_counts['외국어고']\n",
    "\n",
    "##########################################################################################################\n",
    "# 검색 결과 출력 창\n",
    "# '중요단어 카운팅 수'기반 [결과 출력 창]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# '중요단어 카운팅 수'에 대한 구간 설정\n",
    "bins = np.arange(0, filtered_df['중요단어 카운팅 수'].max() + 6, 5)\n",
    "# 구간별로 치환된 값을 새로운 열에 저장\n",
    "filtered_df['가중치_중요단어카운팅수'] = pd.cut(filtered_df['중요단어 카운팅 수'], bins, labels=False) + 1\n",
    "\n",
    "# 'view' 열의 값을 숫자형으로 변환\n",
    "filtered_df['view'] = pd.to_numeric(filtered_df['view'], errors='coerce')\n",
    "\n",
    "# '3. 내신/수능점수' 열의 값을 숫자형으로 변환\n",
    "filtered_df['3. 내신/수능점수'] = pd.to_numeric(filtered_df['3. 내신/수능점수'], errors='coerce')\n",
    "\n",
    "# 'view' 열의 값을 50으로 나눈 뒤 올림하여 1로 치환\n",
    "filtered_df['가중치_조회수'] = np.ceil(filtered_df['view'] / 200)\n",
    "\n",
    "# 'rounded_average'와 '3. 내신/수능점수' 열의 차이 계산\n",
    "filtered_df['가중치_성적'] = filtered_df['3. 내신/수능점수'].apply(pd.to_numeric, errors='coerce')\n",
    "filtered_df['가중치_성적'] = abs(filtered_df['가중치_성적'] - rounded_average)\n",
    "\n",
    "\n",
    "# 1, 2, 3 열 값을 더하여 새로운 열 생성\n",
    "filtered_df['합계'] = filtered_df['가중치_중요단어카운팅수'] + filtered_df['가중치_조회수']+filtered_df['가중치_성적']\n",
    "+ filtered_df['가중치_성적']\n",
    "\n",
    "# 중요단어 카운팅 수와 no 열을 기준으로 중복 제거\n",
    "filtered_df = filtered_df.sort_values(by=['중요단어 카운팅 수'], ascending=False)\n",
    "filtered_df = filtered_df.drop_duplicates(subset=['중요단어 카운팅 수', 'no'], keep='first')\n",
    "\n",
    "\n",
    "\n",
    "print(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\")\n",
    "print(f\"[{user_input}에 게시글들에 대한 통합 결과입니다]\")\n",
    "print(f\"{user_input}의 합격 수기를 작성한 이들의 내신 평균 점수:\"+str(rounded_average))\n",
    "print(f\"{user_input}의 합격 수기를 작성한 이들의 고등학교 유형 분류 결과: \")\n",
    "for school_type in school_types:\n",
    "    count = type_counts.get(school_type, 0)\n",
    "    percentage = (count / total_count) * 100\n",
    "    print(f'{school_type}: {count}개 ({percentage:.2f}%)')\n",
    "keywords()\n",
    "\n",
    "print(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\")\n",
    "print(\"\")\n",
    "for i, row in enumerate(filtered_df.head(3).iterrows(), start=1):\n",
    "    print(f\"[{user_input}에 대한 {i}번째 게시글입니다.]\")\n",
    "    print(f\"1. 글 제목: <{row[1]['title']}>\")\n",
    "    print(f\"2. 작성 날짜: {row[1]['date']}\")\n",
    "    print(f\"3. 출신 고교: {row[1]['2. 출신고교 종류']}\")\n",
    "    print(f\"4. 내신 점수: {row[1]['3. 내신/수능점수']}\")\n",
    "    print(f\"5. 입시 결과: \\n{row[1]['new_col']}\",end=\"\")\n",
    "    print(f\"6. 스펙 핵심: {row[1]['스펙_핵심_추출']}\")\n",
    "    print(f\"6. 스펙 요약: {row[1]['4. 요약']}\")\n",
    "    print(f\"7. 후배들에게 전하고 싶은 말: {row[1]['5. 요약']}\")\n",
    "    print(\"\")\n",
    "    print(\"ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\")\n",
    "    print(\"\")\n",
    "print(f\"[{user_input}의 3개의 전체 요약본 입니다.]\")\n",
    "print(\"\")\n",
    "def summarize(sent_num=3):\n",
    "    summary = []\n",
    "    index = []\n",
    "    for idx in sorted_sent_rank_idx[:sent_num].copy():\n",
    "        index.append(idx)\n",
    "\n",
    "    index.sort()\n",
    "\n",
    "    for idx in index:\n",
    "        summary.append(sentences[idx])\n",
    "\n",
    "    return summary\n",
    "\n",
    "def find_no_values(summaries, df):\n",
    "    no_values = []\n",
    "    for summary in summaries:\n",
    "        found = False  # 요약문이 발견되었는지 여부를 나타내는 변수\n",
    "        for index, row in df.iterrows():\n",
    "            if summary in row['4. 스펙']:\n",
    "                no_values.append(row['no'])\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            no_values.append(None)  # 요약문이 발견되지 않은 경우 None 값을 추가\n",
    "    return no_values\n",
    "\n",
    "\n",
    "# summarize() 함수를 사용하여 요약문 생성\n",
    "summaries = summarize()\n",
    "\n",
    "# 요약문과 해당하는 \"no\" 값을 출력\n",
    "no_values = find_no_values(summaries, filtered_df)\n",
    "for i in range(len(summaries)):\n",
    "    print(\"요약문:\", summaries[i])\n",
    "    print()\n",
    "# summarize() 함수를 사용하여 요약문 생성\n",
    "summaries = summarize()\n",
    "\n",
    "# 요약문과 해당하는 \"no\" 값을 출력하지 않음\n",
    "no_values = find_no_values(summaries, filtered_df)\n",
    "for i in range(len(summaries)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80422386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
